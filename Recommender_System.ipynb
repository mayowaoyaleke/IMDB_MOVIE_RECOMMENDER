{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOMY1XFrOgxgXQvjgjcETDi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayowaoyaleke/IMDB_MOVIE_RECOMMENDER/blob/main/Recommender_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Definition"
      ],
      "metadata": {
        "id": "ZggKUWjkOu5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rapid growth of data collection has led to a new era of information. Data is being used to create more efficient systems and this is where Recommendation Systems come into play. Recommendation Systems are a type of information filtering systems as they improve the quality of search results and provides items that are more relevant to the search item or are realted to the search history of the user.\n",
        "\n",
        "They are used to predict the rating or preference that a user would give to an item. Almost every major tech company has applied them in some form or the other: Amazon uses it to suggest products to customers, YouTube uses it to decide which video to play next on autoplay, and Facebook uses it to recommend pages to like and people to follow. Moreover, companies like Netflix and Spotify depend highly on the effectiveness of their recommendation engines for their business and sucees."
      ],
      "metadata": {
        "id": "2WpVvfg1O3Rs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Description"
      ],
      "metadata": {
        "id": "PyiDve9VO38s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These files contain metadata for all 45,000 movies listed in the Full MovieLens Dataset. The dataset consists of movies released on or before July 2017. Data points include cast, crew, plot keywords, budget, revenue, posters, release dates, languages, production companies, countries, TMDB vote counts and vote averages.\n",
        "\n",
        "This dataset also has files containing 26 million ratings from 270,000 users for all 45,000 movies. Ratings are on a scale of 1-5 and have been obtained from the official GroupLens website.\n",
        "\n",
        "This dataset consists of the following files:\n",
        "\n",
        "1. movies_metadata.csv: The main Movies Metadata file. Contains information on 45,000 movies featured in the Full MovieLens dataset. Features include posters, backdrops, budget, revenue, release dates, languages, production countries and companies.\n",
        "\n",
        "2. keywords.csv: Contains the movie plot keywords for our MovieLens movies. Available in the form of a stringified JSON Object.\n",
        "\n",
        "3. credits.csv: Consists of Cast and Crew Information for all our movies. Available in the form of a stringified JSON Object.\n",
        "\n",
        "4. links.csv: The file that contains the TMDB and IMDB IDs of all the movies featured in the Full MovieLens dataset.\n",
        "\n",
        "5. links_small.csv: Contains the TMDB and IMDB IDs of a small subset of 9,000 movies of the Full Dataset.\n",
        "\n",
        "6. ratings_small.csv: The subset of 100,000 ratings from 700 users on 9,000 movies."
      ],
      "metadata": {
        "id": "hG9iU_q4O-vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Dictionary"
      ],
      "metadata": {
        "id": "gx8bHTiBO_K5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The first dataset contains the following features:_ **\n",
        "\n",
        "movie_id - A unique identifier for each movie.\n",
        "\n",
        "cast - The name of lead and supporting actors.\n",
        "\n",
        "crew - The name of Director, Editor, Composer, Writer etc.\n",
        "\n",
        "\n",
        "\n",
        "**The second dataset has the following features:- **\n",
        "\n",
        "budget - The budget in which the movie was made.\n",
        "\n",
        "genre - The genre of the movie, Action, Comedy ,Thriller etc.\n",
        "\n",
        "homepage - A link to the homepage of the movie.\n",
        "\n",
        "id - This is infact the movie_id as in the first dataset.\n",
        "\n",
        "keywords - The keywords or tags related to the movie.\n",
        "\n",
        "original_language - The language in which the movie was made.\n",
        "\n",
        "original_title - The title of the movie before translation or adaptation.\n",
        "\n",
        "overview - A brief description of the movie.\n",
        "\n",
        "popularity - A numeric quantity specifying the movie popularity.\n",
        "\n",
        "production_companies - The production house of the movie.\n",
        "\n",
        "production_countries - The country in which it was produced.\n",
        "\n",
        "release_date - The date on which it was released.\n",
        "\n",
        "revenue - The worldwide revenue generated by the movie.\n",
        "\n",
        "runtime - The running time of the movie in minutes.\n",
        "\n",
        "status - \"Released\" or \"Rumored\".\n",
        "\n",
        "tagline - Movie's tagline.\n",
        "\n",
        "title - Title of the movie.\n",
        "\n",
        "vote_average - average ratings the movie recieved.\n",
        "\n",
        "vote_count - the count of votes recieved."
      ],
      "metadata": {
        "id": "a_-l889KPFkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building\n"
      ],
      "metadata": {
        "id": "urH5JcyyPF4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ],
      "metadata": {
        "id": "8ggo-RpMP3Q4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "NeHqP05TGt3d",
        "outputId": "a820da21-d2e7-4003-e93e-ae0e8f3a4cc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Collecting scikit-surprise\n",
            "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.0/772.0 KB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-surprise->surprise) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-surprise->surprise) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-surprise->surprise) (1.7.3)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp38-cp38-linux_x86_64.whl size=3366460 sha256=375eb0ce80f01ec903079496be86fa66ed6326fb5605d4ea5551ddf6afec7cfd\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/db/86/2c18183a80ba05da35bf0fb7417aac5cddbd93bcb1b92fd3ea\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.3 surprise-0.1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-55a5fc43206d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install surprise'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'evaluate' from 'surprise' (/usr/local/lib/python3.8/dist-packages/surprise/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from ast import literal_eval\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "#!pip install surprise\n",
        "from surprise import Reader, Dataset, SVD\n",
        "\n",
        "\n",
        "import warnings; warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GQI92OvESMOY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}